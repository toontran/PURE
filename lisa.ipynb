{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/mbornoe/lisa-traffic-light-dataset?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 4.21G/4.21G [01:14<00:00, 60.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ttran02/.cache/kagglehub/datasets/mbornoe/lisa-traffic-light-dataset/versions/2\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"mbornoe/lisa-traffic-light-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "database_path = '/home/ttran02/.cache/kagglehub/datasets/mbornoe/lisa-traffic-light-dataset/versions/2/'\n",
    "target_classes = ['go', 'stop', 'warning','background']\n",
    "color_map = {'go':'green', 'stop':'red', 'warning':'yellow','background':'gray'}\n",
    "# rgb_color_map = {'go': (0, 255, 0), 'stop': (255, 0, 0), 'warning': (255, 255, 0)}\n",
    "\n",
    "train_folder_list = [\n",
    "    'dayTrain',\n",
    "#     'daySequence1',\n",
    "#     'daySequence2',\n",
    "#     'sample-dayClip6',\n",
    "#     'nightTrain',\n",
    "#     'nightSequence1',\n",
    "#     'nightSequence2',\n",
    "#     'sample-nightClip1',\n",
    "]\n",
    "\n",
    "n_samples_per_class = 5000\n",
    "\n",
    "def get_annotarion_dataframe(train_data_folders):\n",
    "    data_base_path = '/home/ttran02/.cache/kagglehub/datasets/mbornoe/lisa-traffic-light-dataset/versions/2/'\n",
    "    annotation_list = list()\n",
    "    for folder in [folder + '/' for folder in train_data_folders if os.listdir(data_base_path)]:\n",
    "        annotation_path = ''\n",
    "        if 'sample' not in folder:\n",
    "            annotation_path = data_base_path + 'Annotations/Annotations/' + folder\n",
    "        else:\n",
    "            annotation_path = data_base_path + folder*2\n",
    "        image_frame_path = data_base_path + folder*2\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        if 'Clip' in os.listdir(annotation_path)[0]:\n",
    "            clip_list = os.listdir(annotation_path)\n",
    "            for clip_folder in clip_list:\n",
    "                df = pd.read_csv(annotation_path + clip_folder +  '/frameAnnotationsBOX.csv', sep=\";\")\n",
    "                df['image_path'] = image_frame_path + clip_folder + '/frames/'\n",
    "                annotation_list.append(df)\n",
    "        else:\n",
    "            df = pd.read_csv(annotation_path +  'frameAnnotationsBOX.csv', sep=\";\")\n",
    "            df['image_path'] = image_frame_path + 'frames/'\n",
    "            annotation_list.append(df)\n",
    "        \n",
    "    df = pd.concat(annotation_list)\n",
    "    df = df.drop(['Origin file', 'Origin frame number', 'Origin track', 'Origin track frame number'], axis=1)\n",
    "    df.columns = ['filename', 'target', 'x1', 'y1', 'x2', 'y2', 'image_path']\n",
    "    df = df[df['target'].isin(target_classes)]\n",
    "    df['filename'] = df['filename'].apply(lambda filename: re.findall(\"\\/([\\d\\w-]*.jpg)\", filename)[0])\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ttran02/.cache/kagglehub/datasets/mbornoe/lisa-traffic-light-dataset/versions/2/dayTrain/dayTrain\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dayClip7--02718.jpg</td>\n",
       "      <td>background</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dayClip7--02717.jpg</td>\n",
       "      <td>background</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dayClip7--02716.jpg</td>\n",
       "      <td>background</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dayClip7--02715.jpg</td>\n",
       "      <td>background</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dayClip7--02714.jpg</td>\n",
       "      <td>background</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31058</th>\n",
       "      <td>dayClip1--02160.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>660</td>\n",
       "      <td>322</td>\n",
       "      <td>672</td>\n",
       "      <td>342</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31059</th>\n",
       "      <td>dayClip1--02160.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>777</td>\n",
       "      <td>351</td>\n",
       "      <td>792</td>\n",
       "      <td>376</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31060</th>\n",
       "      <td>dayClip1--02160.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>838</td>\n",
       "      <td>174</td>\n",
       "      <td>868</td>\n",
       "      <td>234</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31061</th>\n",
       "      <td>dayClip1--02160.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>987</td>\n",
       "      <td>213</td>\n",
       "      <td>1014</td>\n",
       "      <td>263</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31062</th>\n",
       "      <td>dayClip1--02160.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>1157</td>\n",
       "      <td>293</td>\n",
       "      <td>1181</td>\n",
       "      <td>338</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31063 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename      target    x1   y1    x2   y2  \\\n",
       "0      dayClip7--02718.jpg  background     0    0     0    0   \n",
       "1      dayClip7--02717.jpg  background     0    0     0    0   \n",
       "2      dayClip7--02716.jpg  background     0    0     0    0   \n",
       "3      dayClip7--02715.jpg  background     0    0     0    0   \n",
       "4      dayClip7--02714.jpg  background     0    0     0    0   \n",
       "...                    ...         ...   ...  ...   ...  ...   \n",
       "31058  dayClip1--02160.jpg          go   660  322   672  342   \n",
       "31059  dayClip1--02160.jpg          go   777  351   792  376   \n",
       "31060  dayClip1--02160.jpg          go   838  174   868  234   \n",
       "31061  dayClip1--02160.jpg          go   987  213  1014  263   \n",
       "31062  dayClip1--02160.jpg          go  1157  293  1181  338   \n",
       "\n",
       "                                              image_path  \n",
       "0      /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "1      /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "2      /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "3      /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "4      /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "...                                                  ...  \n",
       "31058  /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "31059  /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "31060  /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "31061  /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "31062  /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "\n",
       "[31063 rows x 7 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation_df = get_annotarion_dataframe(train_folder_list)\n",
    "    \n",
    "clips = {}\n",
    "img_dir = Path(os.path.join(database_path, 'dayTrain/dayTrain'))\n",
    "print(img_dir)\n",
    "    \n",
    "# Walk through all clip directories\n",
    "for clip_dir in img_dir.glob('dayClip*/frames'):\n",
    "    # Extract clip name from path\n",
    "    clip_name = clip_dir.parents[0].name + '--'  # e.g., 'dayClip13--'\n",
    "\n",
    "    # Get all image files in frames directory\n",
    "    frame_files = [f for f in clip_dir.glob('*.jpg')]\n",
    "\n",
    "    # Extract frame numbers\n",
    "    for frame_path in frame_files:\n",
    "        # Extract frame number from filename (e.g., '00000' from 'dayClip13--00000.jpg')\n",
    "        frame_num = int(frame_path.stem.split('--')[-1])\n",
    "\n",
    "        if clip_name.strip('-') not in clips:\n",
    "            clips[clip_name.strip('-')] = set()\n",
    "        clips[clip_name.strip('-')].add(frame_num)\n",
    "\n",
    "positive_frames = set(train_annotation_df['filename'].unique())\n",
    "for clip_name, frame_nums in clips.items():\n",
    "    for frame_num in frame_nums:\n",
    "        filename = f\"{clip_name}--{frame_num:05d}.jpg\"\n",
    "        if filename not in positive_frames:\n",
    "            train_annotation_df = pd.concat([pd.DataFrame([[filename,'background', 0,0,0,0,f\"{img_dir}/{clip_name}/frames/{filename}\"]], \n",
    "                                                          columns=train_annotation_df.columns), \n",
    "                                             train_annotation_df], \n",
    "                                            ignore_index=True)\n",
    "\n",
    "target_classes = train_annotation_df['target'].unique()\n",
    "target_classes.sort()\n",
    "\n",
    "train_annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ttran02/.cache/kagglehub/datasets/mbornoe/lisa-traffic-light-dataset/versions/2/dayTrain/dayTrain/dayClip7/frames/dayClip7--02706.jpg'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation_df.image_path.iloc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataset(annotation_df, n_samples):\n",
    "    df_resample_list = list()\n",
    "    for target in target_classes:\n",
    "        df = annotation_df[annotation_df['target'] == target].copy()\n",
    "        df_r = resample(df, n_samples=n_samples, random_state=42)\n",
    "        df_resample_list.append(df_r)\n",
    "    return pd.concat(df_resample_list).reset_index(drop=True)\n",
    "\n",
    "train_annotation_df = resample_dataset(train_annotation_df, n_samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "background    5000\n",
       "go            5000\n",
       "stop          5000\n",
       "warning       5000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index, counts = np.unique(train_annotation_df['target'], return_counts=True)\n",
    "colors = [color_map[target] for target in index]\n",
    "plt.bar(index, counts, color=colors);\n",
    "train_annotation_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "background    5000\n",
       "go            5000\n",
       "stop          5000\n",
       "warning       5000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 91, 68])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class TrafficLightDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with traffic light annotations\n",
    "            img_dir: Directory containing all images\n",
    "            transform: Optional transforms to be applied on images\n",
    "            include_negatives: Whether to include frames without traffic lights\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Group annotations by filename\n",
    "        self.grouped_annotations = df.groupby('filename')\n",
    "        \n",
    "        # Get all unique filenames with annotations\n",
    "        self.frames = list(set(df['filename'].unique()))\n",
    "        \n",
    "        # Extract all potential frame numbers and organize by clip\n",
    "#         self.clips = self._organize_clips()\n",
    "        \n",
    "        # Create final list of all samples (positive and negative)\n",
    "#         self.all_samples = self._create_sample_list()\n",
    "        \n",
    "#     def _organize_clips(self):\n",
    "#         \"\"\"Organize frames into clips and identify missing frame numbers\"\"\"\n",
    "#         clips = {}\n",
    "# #         img_dir = Path(os.path.join(database_path, 'dayTrain/dayTrain'))\n",
    "# #         print(img_dir)\n",
    "\n",
    "#         # Walk through all clip directories\n",
    "#         for clip_dir in self.img_dir.glob('dayClip*/frames'):\n",
    "#             # Extract clip name from path\n",
    "#             clip_name = clip_dir.parents[0].name + '--'  # e.g., 'dayClip13--'\n",
    "\n",
    "#             # Get all image files in frames directory\n",
    "#             frame_files = [f for f in clip_dir.glob('*.jpg')]\n",
    "\n",
    "#             # Extract frame numbers\n",
    "#             for frame_path in frame_files:\n",
    "#                 # Extract frame number from filename (e.g., '00000' from 'dayClip13--00000.jpg')\n",
    "#                 frame_num = int(frame_path.stem.split('--')[-1])\n",
    "\n",
    "#                 if clip_name.strip('-') not in clips:\n",
    "#                     clips[clip_name.strip('-')] = set()\n",
    "#                 clips[clip_name.strip('-')].add(frame_num)\n",
    "\n",
    "#         return clips\n",
    "    \n",
    "#     def _create_sample_list(self):\n",
    "#         \"\"\"Create list of all samples, including both positive and negative frames\"\"\"\n",
    "#         samples = []\n",
    "        \n",
    "#         # Add all frames that have traffic lights (positive samples)\n",
    "#         for filename in self.positive_frames:\n",
    "#             clip_name, frame_num = filename.split('--')\n",
    "#             filename = f\"{clip_name}/frames/{clip_name}--{frame_num}\"\n",
    "#             samples.append((filename, True))\n",
    "        \n",
    "#         if self.include_negatives:\n",
    "#             # Add frames without traffic lights (negative samples)\n",
    "#             for clip_name, frame_nums in self.clips.items():\n",
    "#                 for frame_num in frame_nums:\n",
    "#                     filename = f\"{clip_name}/frames/{clip_name}--{frame_num:05d}.jpg\"\n",
    "#                     if filename not in self.positive_frames:\n",
    "#                         samples.append((filename, False))\n",
    "        \n",
    "#         return samples\n",
    "    \n",
    "    def _get_boxes_and_labels(self, filename):\n",
    "#         filename = filename.split('/')[-1]\n",
    "        \n",
    "        \"\"\"Get all bounding boxes and labels for a given image\"\"\"\n",
    "#         if filename not in self.positive_frames:\n",
    "#             return torch.zeros((0, 4)), torch.zeros(0, dtype=torch.long)\n",
    "        \n",
    "        annotations = self.grouped_annotations.get_group(filename)\n",
    "        boxes = torch.tensor([[row.x1, row.y1, row.x2, row.y2] \n",
    "                            for _, row in annotations.iterrows()], dtype=torch.float32)\n",
    "        \n",
    "        # Convert string labels to integers\n",
    "        label_map = {'background': 0, 'stop': 1, 'go': 2, 'warning': 3}\n",
    "        labels = torch.tensor([label_map[label] for label in annotations['target']], \n",
    "                            dtype=torch.long)\n",
    "        \n",
    "        return boxes, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.frames[idx]\n",
    "        \n",
    "#         print(filename)\n",
    "#         import pdb;pdb.set_trace()\n",
    "        \n",
    "        # Load image\n",
    "        clip_name = filename.split('--')[0]\n",
    "        img_path = self.img_dir / clip_name / \"frames\" / filename\n",
    "        print(img_path)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get boxes and labels\n",
    "        boxes, labels = self._get_boxes_and_labels(filename)\n",
    "        \n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64),\n",
    "            'filename': filename,\n",
    "        }\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "class TrafficLightDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, target_size=(64,64), transform=None, \n",
    "                 scaling_factor=14):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with traffic light annotations\n",
    "            img_dir: Directory containing all images\n",
    "            target_size: Tuple of (height, width) for resizing\n",
    "            transform: Optional additional transforms to be applied after resizing\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.target_size = target_size \n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.additional_transform = transform\n",
    "        \n",
    "        # Group annotations by filename\n",
    "        self.grouped_annotations = df.groupby('filename')\n",
    "        \n",
    "        # Get all unique filenames with annotations\n",
    "        self.frames = list(set(df['filename'].unique()))\n",
    "        \n",
    "        # Label mapping\n",
    "        self.label_map = {'background': 0, 'stop': 1, 'go': 2, 'warning': 3}\n",
    "        \n",
    "    def _resize_image_and_boxes(self, image, boxes):\n",
    "        \"\"\"\n",
    "        Resize image and adjust bounding boxes accordingly\n",
    "        \"\"\"\n",
    "        # Get original size\n",
    "        orig_w, orig_h = image.size\n",
    "#         print(\"sf, w, h\", self.scaling_factor, orig_w, orig_h)\n",
    "        target_h, target_w = (orig_w // self.scaling_factor, orig_h // self.scaling_factor) \\\n",
    "                                    if self.scaling_factor else self.target_size\n",
    "        self.target_size = target_h, target_w\n",
    "#         print(\"w, h\", target_h, target_w)\n",
    "        \n",
    "        # Compute scaling factors\n",
    "        w_scale = target_w / orig_w\n",
    "        h_scale = target_h / orig_h\n",
    "        \n",
    "        # Resize image\n",
    "        image = F.resize(image, self.target_size)\n",
    "        \n",
    "        if len(boxes):\n",
    "            # Scale bounding boxes\n",
    "            scaled_boxes = boxes.clone()\n",
    "            scaled_boxes[:, [0, 2]] *= w_scale  # scale x coordinates\n",
    "            scaled_boxes[:, [1, 3]] *= h_scale  # scale y coordinates\n",
    "            \n",
    "            # Clamp boxes to image boundaries\n",
    "            scaled_boxes[:, [0, 2]] = torch.clamp(scaled_boxes[:, [0, 2]], 0, target_w)\n",
    "            scaled_boxes[:, [1, 3]] = torch.clamp(scaled_boxes[:, [1, 3]], 0, target_h)\n",
    "            \n",
    "            return image, scaled_boxes\n",
    "        \n",
    "        return image, boxes\n",
    "    \n",
    "    def _get_boxes_and_labels(self, filename):\n",
    "        \"\"\"Get all bounding boxes and labels for a given image\"\"\"\n",
    "        annotations = self.grouped_annotations.get_group(filename)\n",
    "        boxes = torch.tensor([[row.x1, row.y1, row.x2, row.y2] \n",
    "                            for _, row in annotations.iterrows()], dtype=torch.float32)\n",
    "        \n",
    "        labels = torch.tensor([self.label_map[label] for label in annotations['target']], \n",
    "                            dtype=torch.long)\n",
    "        \n",
    "        return boxes, labels\n",
    "    \n",
    "    def _validate_boxes(self, boxes):\n",
    "        \"\"\"Remove invalid boxes (those with zero width or height after resizing)\"\"\"\n",
    "        if len(boxes) == 0:\n",
    "            return boxes, torch.zeros(0, dtype=torch.long)\n",
    "        \n",
    "        widths = boxes[:, 2] - boxes[:, 0]\n",
    "        heights = boxes[:, 3] - boxes[:, 1]\n",
    "        valid_boxes = (widths > 1) & (heights > 1)\n",
    "        \n",
    "        return boxes[valid_boxes], valid_boxes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.frames[idx]\n",
    "        \n",
    "        # Load image\n",
    "        clip_name = filename.split('--')[0]\n",
    "        img_path = self.img_dir / clip_name / \"frames\" / filename\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get original boxes and labels\n",
    "        boxes, labels = self._get_boxes_and_labels(filename)\n",
    "        \n",
    "        # Resize image and adjust boxes\n",
    "        image, boxes = self._resize_image_and_boxes(image, boxes)\n",
    "        \n",
    "        # Validate boxes after resizing\n",
    "        boxes, valid_indices = self._validate_boxes(boxes)\n",
    "        if len(valid_indices) < len(labels):\n",
    "            labels = labels[valid_indices]\n",
    "        \n",
    "        # Apply any additional transforms\n",
    "        if self.additional_transform:\n",
    "            image = self.additional_transform(image)\n",
    "        \n",
    "        # Convert PIL to tensor if not done by transforms\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = F.to_tensor(image)\n",
    "        \n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64),\n",
    "            'filename': filename,\n",
    "        }\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Example usage:\n",
    "def create_traffic_light_data_loader(\n",
    "    df, img_dir, batch_size=4, transform=None, include_negatives=True, num_workers=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Create data loader for traffic light detection\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    dataset = TrafficLightDataset(\n",
    "        df=df,\n",
    "        img_dir=img_dir,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Custom collate function to handle variable number of objects\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "# Example transforms\n",
    "def get_transforms(train=True):\n",
    "    from torchvision import transforms\n",
    "    \n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# Usage example:\n",
    "# \"\"\"\n",
    "# Create train loader\n",
    "train_loader = create_traffic_light_data_loader(\n",
    "    df=train_annotation_df,\n",
    "    img_dir='/home/ttran02/.cache/kagglehub/datasets/mbornoe/lisa-traffic-light-dataset/versions/2/dayTrain/dayTrain',\n",
    "    batch_size=1,\n",
    "    transform=get_transforms(train=True),\n",
    ")\n",
    "\n",
    "# Iterate through batches\n",
    "for images, targets in train_loader:\n",
    "    # images is a list of tensor images\n",
    "    # targets is a list of dictionaries containing:\n",
    "    #   - boxes: tensor of shape (num_objects, 4)\n",
    "    #   - labels: tensor of shape (num_objects,)\n",
    "    #   - image_id: tensor of shape (1,)\n",
    "    #   - area: tensor of shape (num_objects,)\n",
    "    #   - iscrowd: tensor of shape (num_objects,)\n",
    "    print(images[0].shape)\n",
    "    break\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TrafficLightDataset at 0x7fb076a5c4c0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>dayClip13--00254.jpg</td>\n",
       "      <td>background</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>dayClip13--00254.jpg</td>\n",
       "      <td>background</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>dayClip13--00254.jpg</td>\n",
       "      <td>background</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>dayClip13--00254.jpg</td>\n",
       "      <td>background</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ttran02/.cache/kagglehub/datasets/mborno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename      target  x1  y1  x2  y2  \\\n",
       "1210  dayClip13--00254.jpg  background   0   0   0   0   \n",
       "1559  dayClip13--00254.jpg  background   0   0   0   0   \n",
       "3310  dayClip13--00254.jpg  background   0   0   0   0   \n",
       "4364  dayClip13--00254.jpg  background   0   0   0   0   \n",
       "\n",
       "                                             image_path  \n",
       "1210  /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "1559  /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "3310  /home/ttran02/.cache/kagglehub/datasets/mborno...  \n",
       "4364  /home/ttran02/.cache/kagglehub/datasets/mborno...  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation_df.groupby(\"filename\").get_group(\"dayClip13--00254.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[-1.5870, -1.5870, -1.5528,  ..., -0.9877, -0.9877, -1.1760],\n",
       "           [-1.5870, -1.5699, -1.5357,  ..., -0.9705, -0.9877, -1.1589],\n",
       "           [-1.5870, -1.5528, -1.5185,  ..., -0.9363, -0.9705, -1.1247],\n",
       "           ...,\n",
       "           [-1.9809, -1.9124, -1.8782,  ..., -1.5528, -1.6898, -1.7754],\n",
       "           [-2.0323, -2.0152, -1.9638,  ..., -1.2617, -1.5014, -1.6898],\n",
       "           [-2.0323, -1.9980, -1.9467,  ..., -1.2445, -1.3302, -1.4500]],\n",
       "  \n",
       "          [[-1.0728, -1.0378, -0.9853,  ..., -0.3550, -0.3901, -0.7927],\n",
       "           [-1.0553, -1.0203, -0.9678,  ..., -0.3375, -0.3550, -0.7402],\n",
       "           [-1.0378, -1.0028, -0.9503,  ..., -0.3025, -0.3375, -0.6877],\n",
       "           ...,\n",
       "           [-1.9307, -1.8606, -1.8256,  ..., -1.5630, -1.6506, -1.7031],\n",
       "           [-1.9482, -1.9307, -1.8782,  ..., -1.3704, -1.5105, -1.6681],\n",
       "           [-1.9482, -1.9132, -1.8606,  ..., -1.3529, -1.3880, -1.4755]],\n",
       "  \n",
       "          [[ 0.6531,  0.7402,  0.8099,  ...,  2.0125,  1.9254,  1.1237],\n",
       "           [ 0.6705,  0.7576,  0.8622,  ...,  2.0648,  1.9254,  1.1934],\n",
       "           [ 0.6879,  0.7751,  0.8797,  ...,  2.1171,  2.0125,  1.3502],\n",
       "           ...,\n",
       "           [-1.5256, -1.4907, -1.3861,  ..., -1.0724, -1.1596, -1.1944],\n",
       "           [-1.5430, -1.5256, -1.4384,  ..., -0.8284, -1.0201, -1.1770],\n",
       "           [-1.4907, -1.4733, -1.4036,  ..., -0.8110, -0.8807, -0.9678]]]),),\n",
       " ({'boxes': tensor([[152.2500, 129.5750, 156.7500, 138.4500]]),\n",
       "   'labels': tensor([2]),\n",
       "   'image_id': tensor([4335]),\n",
       "   'area': tensor([39.9375]),\n",
       "   'iscrowd': tensor([0]),\n",
       "   'filename': 'dayClip12--00004.jpg'},))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def split_dataframe(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a dataframe into train and test sets\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        test_size (float): Proportion of dataset to include in the test split\n",
    "        random_state (int): Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_df, test_df (tuple of pd.DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get indices for train and test sets\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        df.index,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Split the dataframe\n",
    "    train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "    test_df = df.loc[test_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Print split information\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Training samples: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Test samples: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Split the dataframe\n",
    "train_df, test_df = split_dataframe(train_annotation_df)\n",
    "\n",
    "# Optionally save to csv\n",
    "# train_df.to_csv('train.csv', index=False)\n",
    "# test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Monte Carlo Tree Search",
   "language": "python",
   "name": "mcts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
